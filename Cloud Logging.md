## Core Functionality

**Cloud Logging** allows users to:

- **Collect, store, search, analyze, monitor, and alert**Â on log entries and events
- **Automatic ingestion**Â with simple routing, storing, and displaying controls

## Key Tools Integration

- **Log Analytics**: View trends
- **Error Reporting**: Quick problem examination
- **Log Explorer**: Integrated analysis starting point

---

## Four Main Aspects of Logging

### 1.Â **Collection**

- Automatically collect cloud events and configuration changes
- Aggregate and centralize logs at:
    - Organizational level
    - Project level
    - Folder level

### 2.Â **Analysis**

- **Logs Explorer**: Primary analysis tool
- **Log Analytics**: Run queries and analyze data
- Stack traces automatically mapped to error types

### 3.Â **Export Options**

- **Cloud Storage**: Export as files
- **Pub/Sub**: Export as messages for near-real-time analysis
- **BigQuery**: Export into tables for SQL analysis
- **Custom processing**: Via Dataflow for stream processing

### 4.Â **Retention**

**Default Retention Periods:**

- **Data access logs**: 30 days (configurable up to 3,650 days)
- **Admin logs**: 400 days
- **Extended retention**: Export to Cloud Storage or BigQuery

---

## Use Cases by Role

### ðŸ‘¨â€ðŸ’»Â **Developers**

- **Quick start**: Out-of-box system metrics and logs
- **SDK integration**: Popular logging libraries support
- **Real-time capabilities**: Analysis, debugging, troubleshooting
- **Error mapping**: Automatic stack trace to error type mapping

### ðŸ”§Â **Operators**

- **Universal collection**: Telemetry beyond just Google Cloud
- **Centralization**: All logs for users, teams, organizations
- **Control**: Retention periods and log location
- **Cost management**: Understand log volume and costs
- **Alerting**: Set alerts on important application metrics
- **Integration**: Third-party service compatibility

### ðŸ›¡ï¸Â **Security Operations (SecOps)**

- **Access authorization**: Ensure all access is authorized
- **Threat detection**: Identify bad actors in network
- **Audit capabilities**: Comprehensive audit logs
- **Network monitoring**: Network telemetry analysis
- **Streamlined security**: Integrated log analysis approach

---

## Advanced Features

- **Logs-based metrics**: Create and integrate into Cloud Monitoring
- **Dashboard integration**: Connect to monitoring dashboards
- **SLO integration**: Include in service level objectives
- **Custom alerts**: Set up automated alerting

## Key Benefit

Comprehensive logging solution that scales from individual developers to enterprise security operations with flexible retention and powerful analysis capabilities.


---

# Cloud Logging Architecture Summary

## Overview

**Cloud Logging** is a **fully managed service** that stores, searches, analyzes, monitors, and alerts on log data and events from Google Cloud at scale, capable of **ingesting data from thousands of VMs**.

---

## Core Benefits

### Primary Functions:

- **Gather data**Â from various workloads for troubleshooting and understanding application needs
- **Analyze large volumes**Â of data using focused tools
- **Route and store logs**Â to chosen regions/services for compliance and business benefits
- **Get compliance insights**Â through audit and application logs

### User Experience:

- **Logs = most visited section**Â in Google Cloud console
- **Highly transitional**Â indicating importance across many scenarios
- **End users need logs**Â for troubleshooting without data overwhelm
- **Logs are the pulse**Â of workloads and applications

---

## Architecture Components

### 1. Log Collections

**Definition**: Places where log data originates

**Sources**:

- **Google Cloud services**: Compute Engine, App Engine, Kubernetes Engine
- **Your own applications**

### 2. Log Routing

**Component**: **Log Router**

**Function**: Routes log data to destinations using:

- **Inclusion filters**: Determine which logs to include
- **Exclusion filters**: Determine which logs to exclude
- **Combination approach**Â for precise routing control

### 3. Log Sinks

**Definition**: Destinations where log data is stored

#### Supported Sink Types:

**Cloud Logging Log Buckets**:

- **Storage buckets**Â specifically designed for log data
- Optimized for logging use cases

**Pub/Sub Topics**:

- **Route log data**Â to other services
- **Third-party logging solutions**Â integration
- Real-time streaming capabilities

**BigQuery**:

- **Fully-managed, petabyte-scale**Â analytics data warehouse
- **Store and analyze**Â log data at scale
- SQL-based analysis capabilities

**Cloud Storage Buckets**:

- **General storage**Â for log data
- **Log entries stored as JSON files**
- Archive and long-term storage

### 4. Log Analysis

**Purpose**: Provide tools to analyze collected logs

#### Analysis Tools:

**Logs Explorer**:

- **Optimized for troubleshooting**Â use cases
- **Features**:
    - Log streaming
    - Log resource explorer
    - Histogram for visualization
- Primary interface for log investigation

**Error Reporting**:

- **React to critical application errors**
- **Automated error grouping**
- **Automated notifications**
- Proactive error management

**Logs-based Metrics, Dashboards, and Alerting**:

- **Make logs actionable**
- **Understand patterns**Â and trends
- **Create monitoring**Â from log data
- Integration with [[Cloud Monitoring]]

**Log Analytics**:

- **Ad hoc log analysis**Â capabilities
- **Expanded toolset**Â for deeper investigation
- Flexible query and analysis features

---

## Key Architecture Flow

1. **Log data originates**Â from various sources (collections)
2. **Log Router processes**Â and applies filters
3. **Data routes to appropriate sinks**Â based on configuration
4. **Analysis tools provide insights**Â from stored data

## Architecture Benefits

- **Scalable ingestion**Â from thousands of sources
- **Flexible routing**Â to multiple destinations simultaneously
- **Multiple analysis approaches**Â for different use cases
- **Compliance-ready**Â storage and routing options
- **Integration-friendly**Â with third-party tools and services

## Key Takeaway

Cloud Logging architecture provides a **comprehensive, scalable logging pipeline** that handles everything from **data collection through analysis**, enabling both **real-time troubleshooting** and **long-term compliance** while supporting diverse integration scenarios.

---

# Log Collection

## Log Categories

### Platform Logs

- **Written by Google Cloud services**
- **Purpose**: Debug, troubleshoot issues, understand Google Cloud services
- **Example**: VPC Flow Logs record network flows from/to VM instances

### Component Logs

- **Generated by Google-provided software components**Â running on user systems
- **Similar to platform logs**Â but from user-deployed Google components
- **Example**: GKE software components running on user VMs or data centers
- **Function**: GKE uses logs/metadata for user support

### Security Logs

- **Answer "who did what, where, and when"**
- **Cloud Audit Logs**: Administrative activities and resource access information
- **Access Transparency**: Actions taken by Google staff accessing user content

### User-Written Logs

- **Custom applications and services logs**
- **Written via**:
    - Ops Agent
    - Cloud Logging API
    - Cloud Logging client libraries

### Multi-Cloud and Hybrid-Cloud Logs

- **Multi-cloud**: Logs from other cloud providers (Microsoft Azure)
- **Hybrid-cloud**: Logs from on-premises infrastructure

---

## Log Collection Methods

### Programmatic Collection

- **Client libraries**: Standard approach for application integration
- **Logging agents**: Automated collection from systems

### Alternative Methods

When client libraries/agents unavailable:

- **gcloud logging write**Â command
- **HTTP commands**Â to Cloud Logging API endpointÂ `entries.write`
- **Experimentation and testing**Â scenarios

### Agent-Based Collection

**Benefits**: Applications can use **any established logging framework**

#### Container Environments (GKE, Container-Optimized OS):

- **Automatic collection**Â fromÂ **stdout and stderr**
- No additional configuration required

#### Virtual Machines:

- **Collect from known file locations**
- **Logging services integration**:
    - Windows Event Log
    - journald
    - syslogd

### Serverless Services

#### Cloud Run and Cloud Run Functions:

- **Simple runtime logging by default**
- **Automatic appearance**Â of stdout/stderr logs in Google Cloud console
- **No additional setup**Â required

---

## Key Collection Characteristics

### Automatic vs Manual:

- **Platform/Component/Security logs**: Automatic collection
- **User-written logs**: Require integration setup
- **Serverless**: Automatic for stdout/stderr

### Flexibility:

- **Multiple collection methods**Â support various deployment scenarios
- **Framework-agnostic**Â - works with existing logging frameworks
- **Cross-environment**Â support from cloud to on-premises

### Visibility Factors:

- **Log visibility varies**Â based on Google Cloud resources in use
- **Project/organization scope**Â determines accessible logs

## Key Takeaway

Cloud Logging provides **comprehensive log collection** across all Google Cloud services and user applications through **multiple collection methods**, supporting everything from **automatic platform logging** to **custom application integration** across **cloud, hybrid, and multi-cloud environments**.

---

# Storing, Routing and Exporting Logs - Key Points

## Cloud Logging Architecture Overview

- **Cloud Logging = collection of components**Â exposed through centralized logging API
- **Log Router**: Processes streaming data, reliably buffers, sends to storage/sink locations
- **Optimized for streaming**Â with parallel processing capabilities

---

## Default Log Storage System

### Automatic Buckets Created Per Project:

#### _Required Bucket:

- **Stores**: Admin Activity audit logs, System Event audit logs, Access Transparency logs
- **Retention**: 400 days (cannot be modified)
- **Cost**: No charges applied
- **Management**: Cannot delete or modify

#### _Default Bucket:

- **Stores**: All other logs except those in _Required bucket
- **Retention**: 30 days (unless custom retention rules applied)
- **Cost**: Standard Cloud Logging pricing
- **Management**: Cannot delete, but can disable _Default log sink

---

## Log Storage Statistics Dashboard

### Current Monitoring Metrics:

- **Current total volume**: Logs received since first of current month
- **Previous month volume**: Last calendar month's logs
- **Projected volume by EOM**: Estimated end-of-month volume based on usage
- **Resource type breakdown**: Total usage by resource type
- **Metrics Explorer integration**: Build charts for collected metrics

---

## Log Routing with Sinks

### Purpose:

- **Forward copies**Â of log entries to non-default locations
- **Use cases**: Extended storage, SQL querying, access control

### Sink Destination Options:

#### Cloud Logging Bucket:

- **Pre-separate log entries**Â into distinct storage buckets

#### BigQuery Dataset:

- **SQL query power**Â for large and complex log entries analysis

#### Cloud Storage Bucket:

- **Simple external storage**Â for long-term retention or external processing

#### Pub/Sub Topic:

- **Export to message handling**Â third-party applications
- **Integration with**: Dataflow, Cloud Run functions

#### Splunk:

- **Integrate into existing**Â Splunk-based systems

#### Other Project:

- **Control access**Â to log entry subsets

---

## Sink Creation Process

### Steps:

1. **Write query**Â in Logs Explorer to select desired log entries
2. **Choose destination**Â (Cloud Storage, BigQuery, Pub/Sub)
3. **Create sink object**Â containing query and destination
4. **Can be created at**: Project, organization, folder, billing account levels

---

## Log Exclusions

### Process:

1. **Build query**Â in Logs Explorer for logs to exclude
2. **Save query**Â for exclusion building
3. **Create exclusion filter**Â to filter unwanted entries
4. **Name exclusion**Â and add filter criteria
5. **Takes effect immediately**
6. **âš ï¸ Warning**: Excluded events lost forever

---

## Export Processing Patterns

### Pub/Sub â†’ Dataflow â†’ BigQuery:

- **Real-time log processing**Â at scale
- **Dataflow reacts**Â to real-time issues
- **Streams logs**Â to BigQuery for long-term analysis

### Cloud Storage Export:

- **Best for**: Long-term retention, reduced costs, configurable lifecycles
- **Features**: Automated storage class changes, auto-delete, guaranteed retention

### Splunk Integration:

- **Two options**:
    - Stream via Pub/Sub to Splunk Dataflow
    - Splunk Add-on for Google Cloud
- **SIEM integration**Â for third-party security tools

---

## Centralized Log Aggregation

### Three Aggregation Levels:

#### Project-Level Sink:

- **Exports logs**Â for specific project
- **Log filters**Â can include/exclude log types

#### Folder-Level Sink:

- **Aggregates folder-level**Â logs
- **Includes children resources**Â (subfolders, projects)

#### Organization-Level Sink:

- **Global view**Â aggregation
- **Includes all children resources**Â (subfolders, projects, billing accounts)

---

## Security Analytics Integration

### Workflow:

1. **Create aggregate sinks**Â for security log collection
2. **Route logs**Â to security analytics tools:
    - Log Analytics
    - BigQuery
    - Chronicle
    - Third-party SIEM technology
3. **Aggregates from organization**Â including contained resources

---

## BigQuery Integration Details

### Field Naming Conventions:

- **LogEntry type fields**: Same names as log entry fields
- **User-supplied fields**: Normalized to lowercase, naming preserved
- **Structured payloads**: Lowercase normalization, naming preserved (without @type)
- **@type payloads**: Special documentation reference required

---

## Key Takeaway:

Cloud Logging provides **flexible routing and export capabilities** through sinks, supporting **multiple destinations and aggregation levels** for diverse use cases from **real-time processing to long-term compliance** while maintaining **cost-effective default storage** with automatic bucket management.


---


# Querying and Viewing Logs

## Logs Explorer Interface Components

### Action Toolbar:

- **Refine logs**Â to projects or storage views
- **Share links**Â to queries
- **Learn about**Â Logs Explorer functionality

### Query Pane:

- **Build queries**Â interactively
- **View recently viewed and saved queries**
- **Additional query management**Â features

### Results Toolbar:

- **Show/hide logs**Â and histogram pane
- **Create log-based metrics**Â or alerts
- **Jump to now**Â option for current time results

### Query Results:

- **Details of results**Â with summary and timestamps
- **Troubleshooting support**Â information

### Log Fields Pane:

- **Filter options**Â by various factors:
    - Resource type
    - Log name
    - Project ID
    - Other metadata

### Histogram:

- **Visualize query results**Â as histogram bars
- **Time range representation**Â per bar
- **Color coding**Â based on severity levels

---

## Query Creation Methods

### Four Ways to Create Queries:

1. **Direct LQL**Â (Logging Query Language) writing
2. **Drop-down menus**Â in query builder
3. **Logs field explorer**Â navigation
4. **Clicking fields**Â in results themselves

### Query Builder Drop-Down Options:

#### Resource:

- **Specify resource.type**
- **Single resource selection**Â at a time
- **Uses AND**Â logical operator for entries

#### Log Name:

- **Specify logName**
- **Multiple log names**Â selection allowed
- **Uses OR**Â logical operator for multiple entries

#### Severity:

- **Specify severity levels**
- **Multiple severity levels**Â selection allowed
- **Uses OR**Â logical operator for multiple entries

---

## Advanced Query Operations

### Comparison Operators:

#### Equal/Not Equal:

- **Filter values**Â that match/don't match field values
- **Useful for**: Specific resource types or IDs

#### Numeric Ordering:

- **Filter by timestamp**Â or duration
- **Handy for**: Time-based log searches

#### Colon Operation (:):

- **Check if value exists**
- **Useful for**: Substring matching within log entry fields
- **`:*`Â comparison**: Test if field exists without testing specific value

---

## Time Range Filtering

### Time Range Options:

- **Pre-created choices**Â for common ranges
- **Custom range**Â setting
- **Jump to particular time**Â functionality

### Strategy:

- **Start with known timeframe**Â when looking for specific log entries
- **Narrow to specific time range**Â to improve query performance

---

## Log Fields Panel Features

### Functionality:

- **High-level summary**Â of log data
- **More efficient query refinement**Â method
- **Count of log entries**Â sorted by decreasing count
- **Corresponds to histogram time range**

### Usage:

- **Click fields**Â to add to Query builder
- **Incremental loading**Â as log entries are scanned
- **Total counts**Â shown after query completion (blue progress bar)

---

## Histogram Visualization

### Benefits:

- **Visualize log distribution**Â over time
- **Easier trend identification**Â in log data
- **Troubleshooting support**Â through visual patterns
- **Severity colors**Â help spot increasing errors

### Interactive Features:

- **Point to bar**Â and select "Jump to time"
- **Drill into narrower time range**
- **New query runs**Â with time-range restriction

---

## Boolean Query Operations

### Supported Expressions:

- **AND, OR, NOT**Â Boolean expressions
- **Join queries**Â with logical operators

### Important Rules:

- **Use ALL CAPS**Â for operator names
- **Precedence order**: NOT (highest) â†’ OR â†’ AND
- **Short-circuit operators**: AND and OR

---

## Query Strategy and Best Practices

### Starting Point:

**Begin with what you know**:

- Log filename
- Resource name
- Message contents (partial)

### Text Search Guidelines:

#### Full Text Search:

- **Slow but effective**Â for broad searches
- **Example**:Â `/score called`

#### Restricted Text Search (Better Performance):

- **Restrict to entry region**:Â `jsonPayload:"/score called"`
- **Even better**:Â `jsonPayload.message="/score called"`

### SEARCH Function Usage:

#### Two Forms:

1. **`SEARCH([query])`**: Search entire log entry
2. **`SEARCH([field], [query])`**: Search specific field

#### Requirements:

- **Query argument**Â must be formatted as string literal
- **More efficient**Â than global or substring searches
- **Cannot match non-text fields**

---

## Performance Optimization Tips

### High-Performance Searches:

1. **Search indexed fields**:
    
    - Log entry name
    - Resource type
    - Resource labels
2. **Apply resource constraints**:
    
    ```
    resource.type = "gke_cluster" AND 
    resource.labels.namespace = "my-cool-namespace"
    ```
    
3. **Be specific with log names** - reference logs by name
    
4. **Limit time range** to reduce queried data volume
    

### Why These Work:

- **Preferentially indexed fields**Â in storage
- **Huge difference**Â in query performance
- **Reduced data scanning**Â improves response time

---

## Key Query Strategy:

1. **Start specific**Â with known information
2. **Use indexed fields**Â whenever possible
3. **Restrict time ranges**Â appropriately
4. **Leverage visual tools**Â (histogram, fields panel)
5. **Iteratively refine**Â queries for precision

## Key Takeaway:

Effective log querying combines **strategic use of indexed fields, appropriate time ranges, and visual exploration tools** to quickly find relevant log entries while **optimizing performance through specific, constrained searches** rather than broad text searches.


---


# Log-Based Metrics

## Overview and Definition

### What Are Log-Based Metrics?

- **Derive metric data**Â from log entry content
- **Transform logs**Â into time series data for Cloud Monitoring
- **Examples**: Track specific messages, extract latency information

### Usage in Cloud Monitoring:

- **Charts creation**Â for visualizing log-derived data
- **Alerting policies**Â based on log patterns
- **Time series analysis**Â from log content

---

## Types of Log-Based Metrics

### System-Defined Log-Based Metrics:

- **Provided by Cloud Logging**Â for all Google Cloud projects
- **Calculated only**Â from ingested logs
- **Excluded logs**Â not included in metrics
- **All are counter type**

### User-Defined Log-Based Metrics:

- **Created by users**Â for project-specific tracking
- **Custom filtering**Â with specific log queries
- **Three types**: Counter, Distribution, Boolean

---

## Use Cases for Log-Based Metrics

### When to Use:

1. **Count message occurrences**Â (warnings, errors) with threshold notifications
2. **Observe data trends**Â (latency values) with change notifications
3. **Create charts**Â displaying numeric data from logs
4. **Custom monitoring**Â for business-specific log patterns

---

## IAM Roles and Permissions

### Logging Side:

- **Logs Configuration Writers**: List, create, get, update, delete log-based metrics
- **Logs Viewers**: View existing metrics

### Monitoring Side:

- **Monitoring Viewers**: Read time series in log-based metrics

### Broad Permissions:

- **Logging Admins, Editors, Owners**: Can create log-based metrics

---

## Metric Types Explained

### Counter Metrics:

- **Count log entries**Â matching advanced logs query
- **Example**: Count "/score called" entries
- **Simple numerical counting**

### Distribution Metrics:

- **Record statistical distribution**Â of extracted values
- **Histogram buckets**Â for value distribution
- **Includes**: Count, mean, sum of squared deviations
- **Individual values not recorded**

### Boolean Metrics:

- **Record whether**Â log entry matches specified filter
- **True/false tracking**Â for log conditions

---

## Metric Scope Levels

### System-Defined Metrics:

- **Project level**Â application only
- **Calculated by Log Router**
- **Apply to project**Â where logs received

### User-Defined Metrics:

#### Project-Level:

- **Similar to system-defined**Â metrics
- **Apply only**Â to receiving project logs

#### Bucket-Scoped:

- **Apply to logs**Â in specific log bucket
- **Regardless of**Â originating project
- **Useful for**:
    - Cross-project log routing
    - Aggregated sink scenarios

---

## Creating Log-Based Metrics Workflow

### Basic Flow:

1. **Find logs**Â with requisite data
2. **Filter to required entries**
3. **Create metric**Â using interface
4. **Pick metric type**Â (Counter/Distribution)
5. **Set configurations**Â (if Distribution)
6. **Add labels**Â as needed

### Starting Point:

- UseÂ **Query builder**Â to access project logs
- **Locate relevant entries**Â in log list
- **Filter by clicking**Â log content and "Show matching entries"

---

## Labels in Log-Based Metrics

### Purpose of Labels:

- **Group-by and filtering**Â tasks in Cloud Monitoring
- **Multiple time series**Â support (one per label value)
- **Enhanced metric organization**

### Label Types:

- **Default labels**: Automatically included
- **User-defined labels**: Created via extractor expressions

### Extractor Expression Options:

1. **Entire contents**Â of named LogEntry field
2. **Part of field**Â matching regular expression (regexp)

### Extractable Fields:

- **LogEntry built-in fields**:Â `httpRequest.status`
- **Payload fields**:Â `textPayload`,Â `jsonPayload`,Â `protoPayload`

---

## Label Limitations and Considerations

### Key Constraints:

- **Maximum 10**Â user-defined labels per metric
- **Cannot remove**Â metrics once created
- **~30,000 active time series**Â limit per metric
- **Each label**Â significantly grows time series count

### Calculation Example:

- **100 resources**Â (VM instances)
- **20 possible label values**
- **Result**: Up toÂ **2,000 time series**Â for metric

### Label Creation Requirements:

#### Required Fields:

- **Name**: Identifier for Monitoring
- **Description**: Specific label description
- **Label Type**: String, Boolean, or Integer
- **Field Name**: Log entry field containing label value (supports autocomplete)

#### Optional Field:

- **Extraction Regular Expression**:
    - **Leave empty**Â if using entire field contents
    - **Specify regexp**Â to extract partial field value

---

## Example Application Context

### Sample NodeJS Application:

- **Express web server**Â on Cloud Run
- **Watches '/score' path**Â requests
- **Generates random score**Â (1-100)
- **Creates log entry**Â with:
    - "/score called" text
    - Random score
    - Container ID
    - Fun factor
- **Returns score**Â to browser

### Log Entry Content:

Contains structured data suitable for **metric extraction** and **label creation** from various fields.

---

## Best Practices

### Label Design:

- **Label with care**Â - consider time series impact
- **Be specific**Â in descriptions
- **Plan label cardinality**Â to avoid limits
- **Use appropriate data types**Â for label values

### Metric Planning:

- **Consider long-term needs**Â (cannot remove metrics)
- **Evaluate time series growth**Â before adding labels
- **Test with sample data**Â before production deployment

## Key Takeaway:

Log-based metrics provide **powerful transformation of log data into actionable monitoring metrics** with **flexible labeling and scoping options**, but require **careful planning of label cardinality** and **understanding of time series limitations** for effective implementation.

---

# Log Analytics 

## Overview and Core Functionality

### What is Log Analytics?

- **New feature**Â in Cloud Logging
- **BigQuery analytical power**Â within Cloud Logging console
- **Optimized user interface**Â for log analysis
- **Dual access**: Log Analytics UI + BigQuery integration

### Key Capability:

- **Activate analytics**Â on log buckets
- **Automatic data availability**Â in both interfaces
- **No separate data routing**Â to BigQuery required
- **Standard Cloud Logging querying**Â still available with LQL

---

## Architecture and Data Flow

### Log Collection Path:

1. **Logs written** to Logging API via:
    
    - Client libraries
    - stdout/fluentbit agent
    - Direct API calls
2. **Logs Explorer**: Troubleshooting with search, filter, histogram, suggested search
    
3. **Log Router**: Routes logs to Logging Sink for Logs Bucket
    
4. **Log Analytics**: Analyzes performance, data access, network patterns
    
5. **BigQuery Integration**: Maps logs to BigQuery tables with data types (JSON, STRING, INT64, RECORD)
    

---

## Analytics-Enabled Buckets vs Traditional Export

### Key Differences from Traditional BigQuery Export:

#### Management:

- **Log data managed**Â by Cloud Logging (not user-managed)
- **Costs included**Â in Logging pricing (not separate BigQuery costs)
- **Data residency and lifecycle**Â handled by Cloud Logging

#### Access Methods:

- **Direct querying**Â in Cloud Logging via Log Analytics UI
- **Read-only BigQuery view**Â for combining with other datasets
- **Toggleable BigQuery access**Â (can enable/disable connection)

#### UI Optimization:

- **Log Analytics UI optimized**Â for unstructured log data viewing
- **Better suited**Â for log-specific analysis tasks

---

## Creating Analytics-Enabled Buckets

### Console Process:

1. **Navigate to**Â Logs Storage
2. **Click**Â "Create log bucket"
3. **Select**Â "Upgrade to use Log Analytics"

### Important Note:

- **Upgrading is permanent**Â - cannot downgrade bucket
- **Cannot remove**Â Log Analytics once enabled

---

## Use Cases by Role

### DevOps Specialists:

#### Primary Goal:

- **Reduce MTTR**Â (Mean Time to Repair)
- **Quick troubleshooting**Â for issues

#### Log Analytics Capabilities:

- **Count top requests**Â grouped by response type
- **Group by severity**Â levels
- **Rapid issue diagnosis**Â through aggregated data
- **Pattern identification**Â in application performance

### Security Personnel:

#### Primary Need:

- **Find audit logs**Â for specific users over time periods
- **Security attack investigation**

#### Log Analytics Capabilities:

- **Query large volumes**Â of security data
- **Better security investigation**Â tools
- **Historical analysis**Â of user activities
- **Advanced filtering**Â for security events

### IT/Network Operations:

#### Primary Focus:

- **Network issues identification**Â for GKE instances
- **VPC and firewall rules**Â analysis

#### Log Analytics Capabilities:

- **Network insights**Â through log aggregation
- **Advanced network management**Â capabilities
- **VPC-specific analysis**Â tools
- **Firewall rule impact**Â assessment

---

## BigQuery Integration Features

### Dual Access Model:

- **Log Analytics UI**: Optimized for log-specific analysis
- **BigQuery interface**: Standard SQL querying with joins

### Data Joining Capabilities:

- **Combine logs data**Â with other BigQuery datasets
- **Cross-dataset analysis**Â for comprehensive insights
- **Standard BigQuery functionality**Â available

### Access Control:

- **Toggle BigQuery connection**Â on/off
- **Flexible access management**
- **Read-only view**Â prevents accidental data modification

---

## Technical Implementation

### Data Mapping:

- **Automatic schema mapping**Â from logs to BigQuery tables
- **Support for complex data types**:
    - JSON objects
    - STRING fields
    - INT64 numbers
    - RECORD structures

### Cost Management:

- **Integrated pricing**Â with Cloud Logging
- **No separate BigQuery ingestion**Â costs
- **Storage costs included**Â in Logging pricing model

---

## Key Benefits

### Operational Efficiency:

- **Unified interface**Â for log analysis
- **No data duplication**Â management required
- **Faster troubleshooting**Â with optimized UI

### Analytical Power:

- **BigQuery capabilities**Â without complexity
- **Advanced aggregation**Â and analysis
- **Large-scale data processing**

### Flexibility:

- **Multiple access methods**Â for different use cases
- **Role-specific optimization**Â for various teams
- **Seamless integration**Â with existing workflows

---

## Best Practices

### When to Enable:

- **Heavy log analysis**Â requirements
- **Need for complex queries**Â on log data
- **Cross-dataset analysis**Â needs
- **Long-term analytical**Â use cases

### Considerations:

- **Permanent upgrade**Â decision
- **Evaluate long-term needs**Â before enabling
- **Consider team roles**Â and access requirements
- **Plan for integrated**Â vs separate BigQuery usage

## Key Takeaway:

Log Analytics **bridges the gap between operational logging and analytical capabilities** by providing **BigQuery's power within Cloud Logging's interface**, enabling **role-specific analysis workflows** while **simplifying data management** and **eliminating duplicate data routing**.


---

# Cloud Audit Logs

## Overview and Purpose

### Primary Function:

**Answer the question**: "Who did what, where, and when?"

### Coverage:

- **Maintained for**: Each Google Cloud project, folder, and organization
- **Service Coverage**: All Google Cloud services will eventually provide audit logs
- **Current Status**: See Google services with audit logs documentation for details

---

## Four Types of Audit Logs

### 1. Admin Activity Audit Logs

#### What They Record:

- **API calls**Â orÂ **administrative actions**Â that modify configuration/metadata
- **Examples**: Creating VM instances, changing IAM permissions

#### Characteristics:

- **Always enabled**Â (cannot be disabled)
- **Retention**: 400 days
- **Cost**: Available atÂ **no charge**
- **Required IAM roles**: Logging/Logs Viewer or Project/Viewer

### 2. System Event Audit Logs

#### What They Record:

- **Google Cloud administrative actions**Â that modify resource configuration
- **Generated by Google systems**Â (not direct user actions)

#### Characteristics:

- **Always enabled**Â (cannot be disabled)
- **Retention**: 400 days
- **Cost**:Â **Free**
- **Required IAM roles**: Logging/Logs Viewer or Project/Viewer

### 3. Data Access Audit Logs

#### What They Record:

- **API calls**Â that read configuration/metadata of resources
- **User-driven API calls**Â that create, modify, or read user-provided data

#### What They DON'T Record:

- **Publicly shared resources**Â (All Users or All Authenticated Users)
- **Resources accessible**Â without Google Cloud login

#### Characteristics:

- **Disabled by default**Â (except for BigQuery)
- **Retention**: 30 days (default when enabled)
- **Cost**:Â **Chargeable**Â when enabled
- **Required IAM roles**: Logging/Private Logs Viewer or Project/Owner

### 4. Policy Denied Audit Logs

#### What They Record:

- **Access denials**Â when security policy is violated
- **User or service account**Â denied access by Google Cloud services

#### Characteristics:

- **Generated by default**Â (always enabled)
- **Cost**: ProjectÂ **charged for log storage**
- **Cannot be disabled**Â directly
- **Exclusion filters**: Can prevent ingestion/storage in Cloud Logging
- **Required IAM roles**: Same as other audit logs

---

## Accessing and Filtering Audit Logs

### Navigation Steps:

1. **Navigate to**Â Logs Explorer
2. **Filter using**Â Log name drop-down menu
3. **Quick tip**: Type "cloudaudit" in filter box (faster than scrolling)

### Missing Logs:

- **If audit log missing**: Simply means no current entries exist

### Query Building:

- **Use query builder**Â to filter audit logs
- **Auto-populated queries**Â when using UI
- **Documentation available**Â for detailed query syntax

---

## Access Transparency Logs

### Purpose:

- **Different from Cloud Audit Logs**
- **Record actions taken by Google personnel**Â (not automated systems)
- **Human Googler access**Â to your data

### Key Differences:

- **Cloud Audit Logs**: Record actions byÂ **your organization members**
- **Access Transparency**: Record actions byÂ **Google personnel**

### Availability:

- **Enterprise support packages**Â required
- **Near-real-time**Â log event delivery
- **Multiple surfaces**: APIs, Cloud Logging, Security Command Center

### Use Case:

- **Infrastructure management**Â by Google experts (hardware support, ticket resolution)
- **Transparency**Â into Google staff access to customer data

---

## Best Practices Implications

### Cost Management:

- **Admin Activity & System Event**: Free - no cost concerns
- **Data Access**: Disabled by default - enable only when needed
- **Policy Denied**: Always charged - use exclusion filters if necessary

### Retention Planning:

- **Long-term logs**: Admin Activity & System Event (400 days)
- **Short-term logs**: Data Access (30 days default)
- **Plan retention**Â based on compliance requirements

### Access Control:

- **Different IAM requirements**Â for different log types
- **Private logs**Â require higher permissions (Private Logs Viewer/Owner)
- **Standard logs**Â accessible with basic viewer permissions

### Monitoring Strategy:

- **Always-on logs**Â provide baseline security monitoring
- **Optional logs**Â require deliberate activation decisions
- **Exclusion filters**Â help manage costs and noise

---

## Key Implementation Considerations

### Security Monitoring:

- **Comprehensive coverage**Â of "who did what, where, when"
- **Administrative actions**Â always captured
- **Policy violations**Â automatically recorded

### Compliance:

- **400-day retention**Â for administrative logs supports audit requirements
- **Configurable retention**Â for data access logs
- **Multiple log types**Â support different compliance scenarios

### Cost Optimization:

- **Free baseline**Â with Admin Activity and System Event logs
- **Selective enablement**Â of Data Access logs
- **Exclusion filters**Â for managing Policy Denied log costs

## Key Takeaway:

Cloud Audit Logs provide **comprehensive security and compliance monitoring** through **four distinct log types** with **different enablement, retention, and cost characteristics**, enabling organizations to **track all administrative and access activities** while **managing costs** through **selective enablement and filtering strategies**.

---

# Data Access Audit Logs

## Configuration Hierarchy and Levels

### Available Configuration Levels:

- **Organization**Â level
- **Folder**Â level
- **Project**Â level
- **Resource**Â level
- **Billing accounts**Â level

### Configuration Rules:

- **Final configuration**Â =Â **union of all configurations**
- **Inheritance principle**: Cannot disable logs enabled at parent level
- **Example**: Project-level service enablement cannot override organization-level enablement

### Principal Exemptions:

- **Exempt specific users/groups**Â from data access recording
- **Reduces cost and noise**Â from uninteresting log volume

---

## Default State and Enablement

### Default Configuration:

- **Disabled by default**Â for all services
- **Exception**:Â **BigQuery**Â (enabled by default)

### Enablement Scope:

- **Organization, folder, project, or service**Â level configuration
- **Granular control**Â over what information is recorded

---

## Three Types of Data Access Information

### 1. Admin-Read:

- **Records operations**Â that read metadata or configuration
- **Example**: Looking at bucket configurations
- **Focus**: Administrative information access

### 2. Data-Read:

- **Records operations**Â that read user-provided data
- **Example**: Listing and downloading files from Cloud Storage
- **Focus**: Actual data consumption

### 3. Data-Write:

- **Records operations**Â that write user-provided data
- **Example**: Creating new Cloud Storage files
- **Focus**: Data modification and creation

---

## Cost Considerations

### Pricing Structure:

- **$0.50 per gigabyte**Â for ingestion
- **Cost directly proportional**Â to log volume
- **High volume potential**Â from data access operations

### Cost Management Strategies:

- **Principal exemptions**Â for routine/uninteresting access
- **Selective service enablement**Â rather than global enablement
- **Strategic configuration**Â at appropriate hierarchy level

---

## CLI and API Configuration

### gcloud CLI Process:

#### Step 1: Export Current Policy

`gcloud projects get-iam-policy PROJECT_ID --format=yaml > /tmp/policy.yaml`

#### Step 2: Edit Configuration File

- **Edit**Â `/tmp/policy.yaml`Â file
- **Add or modify**Â `auditLogConfigs`Â section
- **Specify services**Â and log types

#### Step 3: Apply New Policy

`gcloud projects set-iam-policy PROJECT_ID /tmp/policy.yaml`

### Configuration Options:

#### Service-Specific Enablement:

- **Enable logging per service**Â (e.g., Cloud Run)
- **Granular control**Â over which services generate logs

#### Global Enablement:

- **Enable logging**Â onÂ **all services**
- **Broader coverage**Â but higher costs

---

## Best Practices and Considerations

### Strategic Enablement:

- **Start with specific services**Â rather than global enablement
- **Monitor costs**Â before expanding scope
- **Consider business requirements**Â for different data types

### Exemption Management:

- **Identify routine access patterns**Â that don't need logging
- **Exempt service accounts**Â for automated operations where appropriate
- **Balance security needs**Â with cost management

### Hierarchy Planning:

- **Configure at appropriate level**Â for organizational needs
- **Remember inheritance rules**Â when planning configurations
- **Test configurations**Â at lower levels before organization-wide deployment

### Volume Management:

- **Understand data access patterns**Â before enabling
- **High-traffic services**Â will generate significant log volume
- **Consider retention policies**Â for cost optimization

---

## Implementation Workflow

### Planning Phase:

1. **Identify critical services**Â requiring audit logging
2. **Assess expected log volume**Â and associated costs
3. **Determine exemption requirements**Â for routine operations
4. **Choose appropriate configuration level**Â in hierarchy

### Configuration Phase:

1. **Export current IAM policies**
2. **Modify audit log configurations**Â for target services
3. **Apply new policies**Â via CLI or API
4. **Verify configuration**Â through testing

### Monitoring Phase:

1. **Track log volume**Â and associated costs
2. **Review exemption effectiveness**
3. **Adjust configurations**Â based on operational needs
4. **Regular policy reviews**Â for optimization

---

## Key Decision Points

### When to Enable:

- **Compliance requirements**Â for data access tracking
- **Security monitoring**Â for sensitive data
- **Forensic capabilities**Â for incident investigation

### When to Use Exemptions:

- **High-volume automated operations**
- **Service account routine access**
- **Known safe operations**Â that create log noise

### Configuration Level Choice:

- **Organization**: For company-wide policies
- **Folder/Project**: For specific business units
- **Service**: For targeted monitoring needs

## Key Takeaway:

Data Access audit logs provide **granular visibility into data operations** with **flexible configuration options** across the **resource hierarchy**, but require **careful cost management** through **strategic enablement, exemption planning, and volume monitoring** to balance **security needs with operational costs**.


---

# Audit Log Best Practices 

## Planning and Implementation Strategy

### Planning First Approach:

1. **Start with planning**Â before implementation
2. **Think hierarchically**: Organization â†’ Folder â†’ Project
3. **Consider organizational structure**Â and common project types
4. **Create test project**Â to experiment with logging behavior
5. **Roll out systematically**Â with automation support

### Testing and Validation:

- **Experiment first**Â to understand logging behavior
- **Validate expectations**Â before organization-wide deployment
- **Use Infrastructure as Code**Â for consistent rollouts

---

## Data Access Audit Logs Trade-offs

### Organization-Level Enablement:

#### Advantages:

- **Detailed information**Â on who accessed, edited, deleted what and when
- **Comprehensive coverage**Â across all resources
- **Centralized policy management**

#### Disadvantages:

- **Large log volume**Â growth
- **Billed per gigabyte**Â - high costs
- **Higher queries per second**Â from increased data access requests

---

## Infrastructure as Code (IaC) Integration

### IaC Definition:

- **Automate infrastructure creation/modification**Â using platforms
- **Configuration files**Â through CI/CD pipelines
- **Treat infrastructure like code**

### Terraform Implementation:

#### Platform Details:

- **Open source**Â from HashiCorp (with paid enterprise option)
- **Not hosted in Google Cloud**Â but installed in Cloud Shell by default

#### State Management Options:

- **Remote**: Cloud Storage or Terraform Cloud
- **Local**: Organization-specific setup or HashiCorp Enterprise service

#### Audit Log Benefits:

- **Track IaC resource provisioning**
- **Control log sink filters**Â at project level
- **Apply default include/exclude filters**Â across projects

---

## Log Storage and Management

### Centralized Log Management:

- **Aggregate organization logs**Â into single Cloud Logging bucket
- **Create user-defined buckets**Â for centralization or subdivision
- **Customize based**Â on compliance and usage requirements

### Storage Configuration:

#### Regional Considerations:

- **Latency requirements**Â for specific regions
- **Compliance requirements**Â for data location
- **Availability requirements**Â in specific regions
- **Configure default storage location**Â for automatic region application

#### Encryption Management:

- **Default**: Google-managed encryption at rest
- **Advanced requirements**: Customer-managed encryption keys (CMEK)
- **Control and manage**Â your own encryption keys

---

## Export Strategy Planning

### Export Planning Framework:

1. **Decide aggregated exports**Â at organization level
2. **Plan project-by-project**Â and folder-by-folder options
3. **Carefully consider filters**Â - both inclusions and exclusions
4. **Remember**: Filters apply toÂ **all logs**, not just exports
5. **Consider full exclusions**Â carefully -Â **excluded entries gone forever**

---

## Security and Access Control

### Side-Channel Data Leakage Prevention:

- **Control who gets access**Â to which types of logs
- **Monitor current project**Â access patterns
- **Decide monitoring scope**: Project-by-project vs. grouped projects

### IAM Controls:

- **Apply appropriate IAM**Â on Google Cloud and exported logs
- **Minimal access principle**Â - only required access
- **Special scrutiny**Â for Data Access audit logs (often contain PII)

### Log Views and Buckets:

- **Log buckets**: Store logs including audit logs
- **Log views**: Control access to logs in buckets
- **Custom log views**: Control access from specific projects/users
- **Protect sensitive data**Â and ensure authorized access only

---

## Role-Based Access Scenarios

### Operational Monitoring Structure:

#### CTO (Organization Admin):

- **Assign permissions**Â to security team and service accounts
- **Global administrative**Â oversight

#### Security Team Permissions:

- **logging.viewer**: View Admin Activity audit logs
- **logging.privateLogViewer**: View Data Access audit logs
- **Organization-level assignment**: Global view permissions
- **Selective IAM**Â for exported data (Cloud Storage/BigQuery)

### Development Team Access:

#### Security Team (Unchanged):

- **Maintains global**Â logging.viewer and logging.privateLogViewer

#### Development Team Permissions:

- **logging.viewer at folder level**: See Admin Activity for development projects
- **logging.privateLogViewer at dev folder**: See Data Access audit logs
- **Important**:Â **Limit test data**Â - avoid actual customer information viewing
- **Control exported log access**Â via Cloud Storage/BigQuery IAM

### External Auditor Access:

#### Dashboard Approach:

- **Provide pre-created dashboards**Â where possible
- **Reduces need**Â for broad access permissions

#### Direct Access Options:

- **Logs Viewer role**Â at organization level for broad access
- **BigQuery Data Viewer**Â for exported datasets
- **Cloud Storage IAM**Â with temporary access URLs support

---

## Data Protection Considerations

### PII Management:

- **Data Access audit logs**Â often contain Personally Identifiable Information
- **Integrate with Sensitive Data Protection**Â to redact PII
- **Applies to both**Â current logs and historical archives

### Access Scenarios:

- **Current Data Access logs**: Real-time PII redaction
- **Historical archives**: PII redaction in Cloud Storage exports
- **Consistent protection**Â across all access methods

---

## Implementation Best Practices Summary

### Strategic Approach:

1. **Plan hierarchically**Â (org â†’ folder â†’ project)
2. **Test before rollout**
3. **Use IaC for consistency**
4. **Consider cost implications**Â of Data Access logs

### Security Focus:

1. **Minimal access principle**
2. **Special attention**Â to PII in Data Access logs
3. **Custom log views**Â for granular control
4. **Regular access reviews**

### Operational Efficiency:

1. **Pre-built dashboards**Â for common use cases
2. **Automated filtering**Â and export strategies
3. **Regional compliance**Â considerations
4. **Encryption management**Â based on requirements

## Key Takeaway:

Successful audit log implementation requires **strategic planning across organizational hierarchy**, **careful cost and security trade-offs**, **robust IAM controls**, and **systematic automation** to ensure **comprehensive logging coverage** while **protecting sensitive data** and **managing operational costs**.